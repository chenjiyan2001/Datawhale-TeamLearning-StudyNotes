{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T15:51:41.823126Z",
     "start_time": "2021-06-19T15:51:41.809164Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "from torch_sparse import SparseTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCNConv\n",
    "&emsp;&emsp;这里贴的是教程里经过覆写后的代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCNConv类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T16:20:47.321045Z",
     "start_time": "2021-06-19T16:20:47.315060Z"
    }
   },
   "outputs": [],
   "source": [
    "class GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNConv, self).__init__(aggr='add', flow='source_to_target')\n",
    "        # \"Add\" aggregation (Step 5).\n",
    "        # flow='source_to_target' 表示消息从源节点传播到目标节点\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "\n",
    "        # Step 1: Add self-loops to the adjacency matrix.\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        # Step 2: Linearly transform node feature matrix.\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Step 3: Compute normalization.\n",
    "        row, col = edge_index\n",
    "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        # Step 4-5: Start propagating messages.\n",
    "        adjmat = SparseTensor(row=edge_index[0], col=edge_index[1], value=torch.ones(edge_index.shape[1]))\n",
    "        return self.propagate(adjmat, x=x, norm=norm, deg=deg.view((-1, 1)))\n",
    "\n",
    "    def message(self, x_j, norm, deg_i):\n",
    "        # x_j has shape [E, out_channels]\n",
    "        # deg_i has shape [E, 1]\n",
    "        # Step 4: Normalize node features.\n",
    "        return norm.view(-1, 1) * x_j * deg_i\n",
    "\n",
    "    def aggregate(self, inputs, index, ptr, dim_size):\n",
    "#         print('self.aggr:', self.aggr)\n",
    "#         print(\"`aggregate` is called\")\n",
    "        return super().aggregate(inputs, index, ptr=ptr, dim_size=dim_size)\n",
    "\n",
    "#     def message_and_aggregate(self, adj_t, x, norm):\n",
    "#         print('`message_and_aggregate` is called')\n",
    "#         # 没有实现真实的消息传递与消息聚合的操作\n",
    "#         return super().message_and_aggregate(adj_t) # NotImplementedError\n",
    "        \n",
    "#     def update(self, inputs, deg):\n",
    "#         print(deg)\n",
    "#         return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T16:20:47.696029Z",
     "start_time": "2021-06-19T16:20:47.657153Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "dataset = Planetoid(root='dataset/Cora', name='Cora')\n",
    "data = dataset[0]\n",
    "\n",
    "net = GCNConv(data.num_features, 64)\n",
    "h_nodes = net(data.x, data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T16:31:10.889062Z",
     "start_time": "2021-06-19T16:31:06.422985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] Loss -684.7436, train acc 0.9786\n",
      "[Epoch 1/200] Loss -686.4333, train acc 0.9786\n",
      "[Epoch 2/200] Loss -688.1229, train acc 0.9786\n",
      "[Epoch 3/200] Loss -689.8124, train acc 0.9786\n",
      "[Epoch 4/200] Loss -691.5021, train acc 0.9786\n",
      "[Epoch 5/200] Loss -693.1917, train acc 0.9786\n",
      "[Epoch 6/200] Loss -694.8809, train acc 0.9786\n",
      "[Epoch 7/200] Loss -696.5703, train acc 0.9786\n",
      "[Epoch 8/200] Loss -698.2595, train acc 0.9786\n",
      "[Epoch 9/200] Loss -699.9489, train acc 0.9786\n",
      "Accuracy: 0.5990\n",
      "[Epoch 10/200] Loss -701.6379, train acc 0.9786\n",
      "[Epoch 11/200] Loss -703.3269, train acc 0.9786\n",
      "[Epoch 12/200] Loss -705.0160, train acc 0.9786\n",
      "[Epoch 13/200] Loss -706.7049, train acc 0.9786\n",
      "[Epoch 14/200] Loss -708.3937, train acc 0.9786\n",
      "[Epoch 15/200] Loss -710.0825, train acc 0.9786\n",
      "[Epoch 16/200] Loss -711.7711, train acc 0.9786\n",
      "[Epoch 17/200] Loss -713.4595, train acc 0.9786\n",
      "[Epoch 18/200] Loss -715.1478, train acc 0.9786\n",
      "[Epoch 19/200] Loss -716.8362, train acc 0.9786\n",
      "Accuracy: 0.5990\n",
      "[Epoch 20/200] Loss -718.5244, train acc 0.9786\n",
      "[Epoch 21/200] Loss -720.2122, train acc 0.9786\n",
      "[Epoch 22/200] Loss -721.9005, train acc 0.9786\n",
      "[Epoch 23/200] Loss -723.5884, train acc 0.9786\n",
      "[Epoch 24/200] Loss -725.2762, train acc 0.9786\n",
      "[Epoch 25/200] Loss -726.9638, train acc 0.9786\n",
      "[Epoch 26/200] Loss -728.6514, train acc 0.9786\n",
      "[Epoch 27/200] Loss -730.3389, train acc 0.9786\n",
      "[Epoch 28/200] Loss -732.0262, train acc 0.9786\n",
      "[Epoch 29/200] Loss -733.7132, train acc 0.9786\n",
      "Accuracy: 0.5980\n",
      "[Epoch 30/200] Loss -735.4005, train acc 0.9786\n",
      "[Epoch 31/200] Loss -737.0872, train acc 0.9786\n",
      "[Epoch 32/200] Loss -738.7740, train acc 0.9786\n",
      "[Epoch 33/200] Loss -740.4602, train acc 0.9786\n",
      "[Epoch 34/200] Loss -742.1471, train acc 0.9786\n",
      "[Epoch 35/200] Loss -743.8335, train acc 0.9786\n",
      "[Epoch 36/200] Loss -745.5196, train acc 0.9786\n",
      "[Epoch 37/200] Loss -747.2057, train acc 0.9786\n",
      "[Epoch 38/200] Loss -748.8916, train acc 0.9786\n",
      "[Epoch 39/200] Loss -750.5775, train acc 0.9786\n",
      "Accuracy: 0.5980\n",
      "[Epoch 40/200] Loss -752.2629, train acc 0.9786\n",
      "[Epoch 41/200] Loss -753.9487, train acc 0.9786\n",
      "[Epoch 42/200] Loss -755.6340, train acc 0.9786\n",
      "[Epoch 43/200] Loss -757.3192, train acc 0.9786\n",
      "[Epoch 44/200] Loss -759.0040, train acc 0.9786\n",
      "[Epoch 45/200] Loss -760.6889, train acc 0.9786\n",
      "[Epoch 46/200] Loss -762.3740, train acc 0.9786\n",
      "[Epoch 47/200] Loss -764.0587, train acc 0.9786\n",
      "[Epoch 48/200] Loss -765.7431, train acc 0.9786\n",
      "[Epoch 49/200] Loss -767.4275, train acc 0.9786\n",
      "Accuracy: 0.5980\n",
      "[Epoch 50/200] Loss -769.1115, train acc 0.9786\n",
      "[Epoch 51/200] Loss -770.7958, train acc 0.9786\n",
      "[Epoch 52/200] Loss -772.4797, train acc 0.9786\n",
      "[Epoch 53/200] Loss -774.1635, train acc 0.9786\n",
      "[Epoch 54/200] Loss -775.8470, train acc 0.9786\n",
      "[Epoch 55/200] Loss -777.5306, train acc 0.9786\n",
      "[Epoch 56/200] Loss -779.2140, train acc 0.9786\n",
      "[Epoch 57/200] Loss -780.8970, train acc 0.9786\n",
      "[Epoch 58/200] Loss -782.5801, train acc 0.9786\n",
      "[Epoch 59/200] Loss -784.2631, train acc 0.9786\n",
      "Accuracy: 0.5980\n",
      "[Epoch 60/200] Loss -785.9456, train acc 0.9786\n",
      "[Epoch 61/200] Loss -787.6285, train acc 0.9786\n",
      "[Epoch 62/200] Loss -789.3109, train acc 0.9786\n",
      "[Epoch 63/200] Loss -790.9933, train acc 0.9786\n",
      "[Epoch 64/200] Loss -792.6755, train acc 0.9786\n",
      "[Epoch 65/200] Loss -794.3575, train acc 0.9786\n",
      "[Epoch 66/200] Loss -796.0395, train acc 0.9786\n",
      "[Epoch 67/200] Loss -797.7211, train acc 0.9786\n",
      "[Epoch 68/200] Loss -799.4028, train acc 0.9786\n",
      "[Epoch 69/200] Loss -801.0841, train acc 0.9786\n",
      "Accuracy: 0.5980\n",
      "[Epoch 70/200] Loss -802.7653, train acc 0.9786\n",
      "[Epoch 71/200] Loss -804.4465, train acc 0.9786\n",
      "[Epoch 72/200] Loss -806.1278, train acc 0.9786\n",
      "[Epoch 73/200] Loss -807.8087, train acc 0.9786\n",
      "[Epoch 74/200] Loss -809.4893, train acc 0.9786\n",
      "[Epoch 75/200] Loss -811.1699, train acc 0.9786\n",
      "[Epoch 76/200] Loss -812.8504, train acc 0.9786\n",
      "[Epoch 77/200] Loss -814.5307, train acc 0.9786\n",
      "[Epoch 78/200] Loss -816.2107, train acc 0.9786\n",
      "[Epoch 79/200] Loss -817.8908, train acc 0.9786\n",
      "Accuracy: 0.5980\n",
      "[Epoch 80/200] Loss -819.5707, train acc 0.9786\n",
      "[Epoch 81/200] Loss -821.2505, train acc 0.9786\n",
      "[Epoch 82/200] Loss -822.9301, train acc 0.9786\n",
      "[Epoch 83/200] Loss -824.6095, train acc 0.9786\n",
      "[Epoch 84/200] Loss -826.2888, train acc 0.9786\n",
      "[Epoch 85/200] Loss -827.9679, train acc 0.9786\n",
      "[Epoch 86/200] Loss -829.6470, train acc 0.9786\n",
      "[Epoch 87/200] Loss -831.3256, train acc 0.9786\n",
      "[Epoch 88/200] Loss -833.0042, train acc 0.9786\n",
      "[Epoch 89/200] Loss -834.6829, train acc 0.9786\n",
      "Accuracy: 0.5980\n",
      "[Epoch 90/200] Loss -836.3615, train acc 0.9786\n",
      "[Epoch 91/200] Loss -838.0397, train acc 0.9786\n",
      "[Epoch 92/200] Loss -839.7178, train acc 0.9786\n",
      "[Epoch 93/200] Loss -841.3956, train acc 0.9786\n",
      "[Epoch 94/200] Loss -843.0738, train acc 0.9786\n",
      "[Epoch 95/200] Loss -844.7515, train acc 0.9786\n",
      "[Epoch 96/200] Loss -846.4287, train acc 0.9786\n",
      "[Epoch 97/200] Loss -848.1064, train acc 0.9786\n",
      "[Epoch 98/200] Loss -849.7834, train acc 0.9786\n",
      "[Epoch 99/200] Loss -851.4608, train acc 0.9786\n",
      "Accuracy: 0.5980\n",
      "[Epoch 100/200] Loss -853.1378, train acc 0.9786\n",
      "[Epoch 101/200] Loss -854.8146, train acc 0.9786\n",
      "[Epoch 102/200] Loss -856.4911, train acc 0.9786\n",
      "[Epoch 103/200] Loss -858.1677, train acc 0.9786\n",
      "[Epoch 104/200] Loss -859.8444, train acc 0.9786\n",
      "[Epoch 105/200] Loss -861.5205, train acc 0.9786\n",
      "[Epoch 106/200] Loss -863.1967, train acc 0.9786\n",
      "[Epoch 107/200] Loss -864.8728, train acc 0.9786\n",
      "[Epoch 108/200] Loss -866.5488, train acc 0.9786\n",
      "[Epoch 109/200] Loss -868.2242, train acc 0.9786\n",
      "Accuracy: 0.5990\n",
      "[Epoch 110/200] Loss -869.9001, train acc 0.9786\n",
      "[Epoch 111/200] Loss -871.5753, train acc 0.9786\n",
      "[Epoch 112/200] Loss -873.2507, train acc 0.9786\n",
      "[Epoch 113/200] Loss -874.9255, train acc 0.9786\n",
      "[Epoch 114/200] Loss -876.6007, train acc 0.9786\n",
      "[Epoch 115/200] Loss -878.2757, train acc 0.9786\n",
      "[Epoch 116/200] Loss -879.9502, train acc 0.9786\n",
      "[Epoch 117/200] Loss -881.6249, train acc 0.9786\n",
      "[Epoch 118/200] Loss -883.2996, train acc 0.9786\n",
      "[Epoch 119/200] Loss -884.9738, train acc 0.9786\n",
      "Accuracy: 0.5970\n",
      "[Epoch 120/200] Loss -886.6481, train acc 0.9786\n",
      "[Epoch 121/200] Loss -888.3221, train acc 0.9786\n",
      "[Epoch 122/200] Loss -889.9960, train acc 0.9786\n",
      "[Epoch 123/200] Loss -891.6697, train acc 0.9786\n",
      "[Epoch 124/200] Loss -893.3434, train acc 0.9786\n",
      "[Epoch 125/200] Loss -895.0170, train acc 0.9786\n",
      "[Epoch 126/200] Loss -896.6904, train acc 0.9786\n",
      "[Epoch 127/200] Loss -898.3635, train acc 0.9786\n",
      "[Epoch 128/200] Loss -900.0364, train acc 0.9786\n",
      "[Epoch 129/200] Loss -901.7095, train acc 0.9786\n",
      "Accuracy: 0.5970\n",
      "[Epoch 130/200] Loss -903.3822, train acc 0.9786\n",
      "[Epoch 131/200] Loss -905.0547, train acc 0.9786\n",
      "[Epoch 132/200] Loss -906.7275, train acc 0.9786\n",
      "[Epoch 133/200] Loss -908.4000, train acc 0.9786\n",
      "[Epoch 134/200] Loss -910.0721, train acc 0.9786\n",
      "[Epoch 135/200] Loss -911.7443, train acc 0.9786\n",
      "[Epoch 136/200] Loss -913.4163, train acc 0.9786\n",
      "[Epoch 137/200] Loss -915.0879, train acc 0.9786\n",
      "[Epoch 138/200] Loss -916.7598, train acc 0.9786\n",
      "[Epoch 139/200] Loss -918.4313, train acc 0.9786\n",
      "Accuracy: 0.5970\n",
      "[Epoch 140/200] Loss -920.1028, train acc 0.9786\n",
      "[Epoch 141/200] Loss -921.7740, train acc 0.9786\n",
      "[Epoch 142/200] Loss -923.4454, train acc 0.9786\n",
      "[Epoch 143/200] Loss -925.1164, train acc 0.9786\n",
      "[Epoch 144/200] Loss -926.7872, train acc 0.9786\n",
      "[Epoch 145/200] Loss -928.4578, train acc 0.9786\n",
      "[Epoch 146/200] Loss -930.1286, train acc 0.9786\n",
      "[Epoch 147/200] Loss -931.7990, train acc 0.9786\n",
      "[Epoch 148/200] Loss -933.4691, train acc 0.9786\n",
      "[Epoch 149/200] Loss -935.1396, train acc 0.9786\n",
      "Accuracy: 0.5970\n",
      "[Epoch 150/200] Loss -936.8097, train acc 0.9786\n",
      "[Epoch 151/200] Loss -938.4795, train acc 0.9786\n",
      "[Epoch 152/200] Loss -940.1494, train acc 0.9786\n",
      "[Epoch 153/200] Loss -941.8187, train acc 0.9786\n",
      "[Epoch 154/200] Loss -943.4886, train acc 0.9786\n",
      "[Epoch 155/200] Loss -945.1578, train acc 0.9786\n",
      "[Epoch 156/200] Loss -946.8273, train acc 0.9786\n",
      "[Epoch 157/200] Loss -948.4963, train acc 0.9786\n",
      "[Epoch 158/200] Loss -950.1656, train acc 0.9786\n",
      "[Epoch 159/200] Loss -951.8340, train acc 0.9786\n",
      "Accuracy: 0.5970\n",
      "[Epoch 160/200] Loss -953.5029, train acc 0.9786\n",
      "[Epoch 161/200] Loss -955.1718, train acc 0.9786\n",
      "[Epoch 162/200] Loss -956.8401, train acc 0.9786\n",
      "[Epoch 163/200] Loss -958.5085, train acc 0.9786\n",
      "[Epoch 164/200] Loss -960.1763, train acc 0.9786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 165/200] Loss -961.8450, train acc 0.9786\n",
      "[Epoch 166/200] Loss -963.5127, train acc 0.9786\n",
      "[Epoch 167/200] Loss -965.1804, train acc 0.9786\n",
      "[Epoch 168/200] Loss -966.8481, train acc 0.9786\n",
      "[Epoch 169/200] Loss -968.5157, train acc 0.9786\n",
      "Accuracy: 0.5970\n",
      "[Epoch 170/200] Loss -970.1830, train acc 0.9786\n",
      "[Epoch 171/200] Loss -971.8505, train acc 0.9786\n",
      "[Epoch 172/200] Loss -973.5174, train acc 0.9786\n",
      "[Epoch 173/200] Loss -975.1845, train acc 0.9786\n",
      "[Epoch 174/200] Loss -976.8513, train acc 0.9786\n",
      "[Epoch 175/200] Loss -978.5182, train acc 0.9786\n",
      "[Epoch 176/200] Loss -980.1843, train acc 0.9786\n",
      "[Epoch 177/200] Loss -981.8514, train acc 0.9786\n",
      "[Epoch 178/200] Loss -983.5176, train acc 0.9786\n",
      "[Epoch 179/200] Loss -985.1839, train acc 0.9786\n",
      "Accuracy: 0.5990\n",
      "[Epoch 180/200] Loss -986.8498, train acc 0.9786\n",
      "[Epoch 181/200] Loss -988.5156, train acc 0.9786\n",
      "[Epoch 182/200] Loss -990.1816, train acc 0.9786\n",
      "[Epoch 183/200] Loss -991.8472, train acc 0.9786\n",
      "[Epoch 184/200] Loss -993.5127, train acc 0.9786\n",
      "[Epoch 185/200] Loss -995.1779, train acc 0.9786\n",
      "[Epoch 186/200] Loss -996.8432, train acc 0.9786\n",
      "[Epoch 187/200] Loss -998.5085, train acc 0.9786\n",
      "[Epoch 188/200] Loss -1000.1735, train acc 0.9786\n",
      "[Epoch 189/200] Loss -1001.8386, train acc 0.9786\n",
      "Accuracy: 0.6000\n",
      "[Epoch 190/200] Loss -1003.5034, train acc 0.9786\n",
      "[Epoch 191/200] Loss -1005.1677, train acc 0.9786\n",
      "[Epoch 192/200] Loss -1006.8326, train acc 0.9786\n",
      "[Epoch 193/200] Loss -1008.4970, train acc 0.9786\n",
      "[Epoch 194/200] Loss -1010.1611, train acc 0.9786\n",
      "[Epoch 195/200] Loss -1011.8254, train acc 0.9786\n",
      "[Epoch 196/200] Loss -1013.4890, train acc 0.9786\n",
      "[Epoch 197/200] Loss -1015.1528, train acc 0.9786\n",
      "[Epoch 198/200] Loss -1016.8164, train acc 0.9786\n",
      "[Epoch 199/200] Loss -1018.4799, train acc 0.9786\n",
      "Accuracy: 0.6000\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "net.train()\n",
    "\n",
    "for epoch in range(200):\n",
    "#     net.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get output\n",
    "    out = net(data.x, data.edge_index)\n",
    "    \n",
    "    # Get loss\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    _, pred = out.max(dim=1)\n",
    "    \n",
    "    # Get predictions and calculate training accuracy\n",
    "    correct = float(pred[data.train_mask].eq(data.y[data.train_mask]).sum().item())\n",
    "    acc = correct / data.train_mask.sum().item()\n",
    "    print('[Epoch {}/200] Loss {:.4f}, train acc {:.4f}'.format(epoch, loss.cpu().detach().data.item(), acc))\n",
    "    \n",
    "    # Backward\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Evaluation on test data every 10 epochs\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        net.eval()\n",
    "        _, pred = net(data.x, data.edge_index).max(dim=1)\n",
    "        correct = float(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "        acc = correct / data.test_mask.sum().item()\n",
    "        print('Accuracy: {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作业"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 请总结MessagePassing基类的运行流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 初始化，包括聚合函数、消息传递流向等（并没有去看MessagePassing类的源码，并不清楚还初始化了什么）\n",
    "2. 初始化得到x和edge_index\n",
    "3. 进行message和aggregate的操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 请复现一个一层的图神经网络的构造，总结通过继承MessagePassing基类来构造，自己的图神经网络类的规范。\n",
    "&emsp;&emsp;我也不知道写的对不对。。但能够正常运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T16:54:30.549953Z",
     "start_time": "2021-06-19T16:54:30.543969Z"
    }
   },
   "outputs": [],
   "source": [
    "class Task2GNN(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Task2GNN, self).__init__(aggr='add', flow='source_to_target')\n",
    "        self.lin1 = torch.nn.Linear(in_channels, out_channels)\n",
    "        self.lin2 = torch.nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        x1 = self.lin1(x)\n",
    "        x2 = self.lin2(x)\n",
    "\n",
    "        row, col = edge_index\n",
    "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        adjmat = SparseTensor(row=edge_index[0],\n",
    "                              col=edge_index[1],\n",
    "                              value=torch.ones(edge_index.shape[1]))\n",
    "\n",
    "        return self.propagate(adjmat, x1=x1, x2=x2, norm=norm, deg=deg.view((-1, 1)))\n",
    "    \n",
    "    def message(self, x1_j, x2_j, norm, deg_i):\n",
    "        return norm.view(-1, 1) * (x1_j + x2_j) * deg_i\n",
    "    \n",
    "    def aggregate(self, inputs, index, ptr, dim_size):\n",
    "        return super().aggregate(inputs, index, ptr=ptr, dim_size=dim_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T16:54:31.126843Z",
     "start_time": "2021-06-19T16:54:31.094929Z"
    }
   },
   "outputs": [],
   "source": [
    "task2gnn = Task2GNN(data.num_features, 64)\n",
    "h_nodes = task2gnn(data.x, data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T16:55:17.131279Z",
     "start_time": "2021-06-19T16:55:10.457115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] Loss 0.0013, train acc 0.0000\n",
      "[Epoch 1/200] Loss -3.3780, train acc 0.7143\n",
      "[Epoch 2/200] Loss -6.7573, train acc 0.9071\n",
      "[Epoch 3/200] Loss -10.1365, train acc 0.9714\n",
      "[Epoch 4/200] Loss -13.5158, train acc 0.9857\n",
      "[Epoch 5/200] Loss -16.8950, train acc 1.0000\n",
      "[Epoch 6/200] Loss -20.2742, train acc 1.0000\n",
      "[Epoch 7/200] Loss -23.6533, train acc 1.0000\n",
      "[Epoch 8/200] Loss -27.0324, train acc 1.0000\n",
      "[Epoch 9/200] Loss -30.4114, train acc 1.0000\n",
      "Accuracy: 0.6070\n",
      "[Epoch 10/200] Loss -33.7904, train acc 1.0000\n",
      "[Epoch 11/200] Loss -37.1692, train acc 1.0000\n",
      "[Epoch 12/200] Loss -40.5480, train acc 1.0000\n",
      "[Epoch 13/200] Loss -43.9267, train acc 1.0000\n",
      "[Epoch 14/200] Loss -47.3053, train acc 1.0000\n",
      "[Epoch 15/200] Loss -50.6839, train acc 1.0000\n",
      "[Epoch 16/200] Loss -54.0623, train acc 1.0000\n",
      "[Epoch 17/200] Loss -57.4406, train acc 1.0000\n",
      "[Epoch 18/200] Loss -60.8188, train acc 1.0000\n",
      "[Epoch 19/200] Loss -64.1969, train acc 1.0000\n",
      "Accuracy: 0.6170\n",
      "[Epoch 20/200] Loss -67.5748, train acc 1.0000\n",
      "[Epoch 21/200] Loss -70.9526, train acc 1.0000\n",
      "[Epoch 22/200] Loss -74.3303, train acc 1.0000\n",
      "[Epoch 23/200] Loss -77.7078, train acc 1.0000\n",
      "[Epoch 24/200] Loss -81.0852, train acc 1.0000\n",
      "[Epoch 25/200] Loss -84.4625, train acc 1.0000\n",
      "[Epoch 26/200] Loss -87.8396, train acc 1.0000\n",
      "[Epoch 27/200] Loss -91.2165, train acc 1.0000\n",
      "[Epoch 28/200] Loss -94.5933, train acc 1.0000\n",
      "[Epoch 29/200] Loss -97.9699, train acc 1.0000\n",
      "Accuracy: 0.6210\n",
      "[Epoch 30/200] Loss -101.3464, train acc 1.0000\n",
      "[Epoch 31/200] Loss -104.7227, train acc 1.0000\n",
      "[Epoch 32/200] Loss -108.0988, train acc 1.0000\n",
      "[Epoch 33/200] Loss -111.4747, train acc 1.0000\n",
      "[Epoch 34/200] Loss -114.8505, train acc 1.0000\n",
      "[Epoch 35/200] Loss -118.2262, train acc 1.0000\n",
      "[Epoch 36/200] Loss -121.6015, train acc 1.0000\n",
      "[Epoch 37/200] Loss -124.9767, train acc 1.0000\n",
      "[Epoch 38/200] Loss -128.3518, train acc 1.0000\n",
      "[Epoch 39/200] Loss -131.7267, train acc 1.0000\n",
      "Accuracy: 0.6210\n",
      "[Epoch 40/200] Loss -135.1013, train acc 1.0000\n",
      "[Epoch 41/200] Loss -138.4758, train acc 1.0000\n",
      "[Epoch 42/200] Loss -141.8501, train acc 1.0000\n",
      "[Epoch 43/200] Loss -145.2242, train acc 1.0000\n",
      "[Epoch 44/200] Loss -148.5981, train acc 1.0000\n",
      "[Epoch 45/200] Loss -151.9719, train acc 1.0000\n",
      "[Epoch 46/200] Loss -155.3454, train acc 1.0000\n",
      "[Epoch 47/200] Loss -158.7187, train acc 1.0000\n",
      "[Epoch 48/200] Loss -162.0919, train acc 1.0000\n",
      "[Epoch 49/200] Loss -165.4648, train acc 1.0000\n",
      "Accuracy: 0.6220\n",
      "[Epoch 50/200] Loss -168.8375, train acc 1.0000\n",
      "[Epoch 51/200] Loss -172.2100, train acc 1.0000\n",
      "[Epoch 52/200] Loss -175.5824, train acc 1.0000\n",
      "[Epoch 53/200] Loss -178.9545, train acc 1.0000\n",
      "[Epoch 54/200] Loss -182.3263, train acc 1.0000\n",
      "[Epoch 55/200] Loss -185.6981, train acc 1.0000\n",
      "[Epoch 56/200] Loss -189.0696, train acc 1.0000\n",
      "[Epoch 57/200] Loss -192.4410, train acc 1.0000\n",
      "[Epoch 58/200] Loss -195.8120, train acc 1.0000\n",
      "[Epoch 59/200] Loss -199.1830, train acc 1.0000\n",
      "Accuracy: 0.6200\n",
      "[Epoch 60/200] Loss -202.5537, train acc 1.0000\n",
      "[Epoch 61/200] Loss -205.9241, train acc 1.0000\n",
      "[Epoch 62/200] Loss -209.2944, train acc 1.0000\n",
      "[Epoch 63/200] Loss -212.6645, train acc 1.0000\n",
      "[Epoch 64/200] Loss -216.0345, train acc 1.0000\n",
      "[Epoch 65/200] Loss -219.4041, train acc 1.0000\n",
      "[Epoch 66/200] Loss -222.7735, train acc 1.0000\n",
      "[Epoch 67/200] Loss -226.1428, train acc 1.0000\n",
      "[Epoch 68/200] Loss -229.5118, train acc 1.0000\n",
      "[Epoch 69/200] Loss -232.8807, train acc 1.0000\n",
      "Accuracy: 0.6210\n",
      "[Epoch 70/200] Loss -236.2493, train acc 1.0000\n",
      "[Epoch 71/200] Loss -239.6178, train acc 1.0000\n",
      "[Epoch 72/200] Loss -242.9859, train acc 1.0000\n",
      "[Epoch 73/200] Loss -246.3540, train acc 1.0000\n",
      "[Epoch 74/200] Loss -249.7218, train acc 1.0000\n",
      "[Epoch 75/200] Loss -253.0894, train acc 1.0000\n",
      "[Epoch 76/200] Loss -256.4569, train acc 1.0000\n",
      "[Epoch 77/200] Loss -259.8240, train acc 1.0000\n",
      "[Epoch 78/200] Loss -263.1910, train acc 1.0000\n",
      "[Epoch 79/200] Loss -266.5576, train acc 1.0000\n",
      "Accuracy: 0.6200\n",
      "[Epoch 80/200] Loss -269.9242, train acc 1.0000\n",
      "[Epoch 81/200] Loss -273.2906, train acc 1.0000\n",
      "[Epoch 82/200] Loss -276.6567, train acc 1.0000\n",
      "[Epoch 83/200] Loss -280.0226, train acc 1.0000\n",
      "[Epoch 84/200] Loss -283.3885, train acc 1.0000\n",
      "[Epoch 85/200] Loss -286.7539, train acc 1.0000\n",
      "[Epoch 86/200] Loss -290.1192, train acc 1.0000\n",
      "[Epoch 87/200] Loss -293.4845, train acc 1.0000\n",
      "[Epoch 88/200] Loss -296.8493, train acc 1.0000\n",
      "[Epoch 89/200] Loss -300.2139, train acc 1.0000\n",
      "Accuracy: 0.6200\n",
      "[Epoch 90/200] Loss -303.5783, train acc 1.0000\n",
      "[Epoch 91/200] Loss -306.9427, train acc 1.0000\n",
      "[Epoch 92/200] Loss -310.3067, train acc 1.0000\n",
      "[Epoch 93/200] Loss -313.6706, train acc 1.0000\n",
      "[Epoch 94/200] Loss -317.0343, train acc 1.0000\n",
      "[Epoch 95/200] Loss -320.3977, train acc 1.0000\n",
      "[Epoch 96/200] Loss -323.7607, train acc 1.0000\n",
      "[Epoch 97/200] Loss -327.1240, train acc 1.0000\n",
      "[Epoch 98/200] Loss -330.4868, train acc 1.0000\n",
      "[Epoch 99/200] Loss -333.8492, train acc 1.0000\n",
      "Accuracy: 0.6200\n",
      "[Epoch 100/200] Loss -337.2117, train acc 1.0000\n",
      "[Epoch 101/200] Loss -340.5738, train acc 1.0000\n",
      "[Epoch 102/200] Loss -343.9358, train acc 1.0000\n",
      "[Epoch 103/200] Loss -347.2975, train acc 1.0000\n",
      "[Epoch 104/200] Loss -350.6591, train acc 1.0000\n",
      "[Epoch 105/200] Loss -354.0205, train acc 1.0000\n",
      "[Epoch 106/200] Loss -357.3816, train acc 1.0000\n",
      "[Epoch 107/200] Loss -360.7424, train acc 1.0000\n",
      "[Epoch 108/200] Loss -364.1033, train acc 1.0000\n",
      "[Epoch 109/200] Loss -367.4637, train acc 1.0000\n",
      "Accuracy: 0.6180\n",
      "[Epoch 110/200] Loss -370.8241, train acc 1.0000\n",
      "[Epoch 111/200] Loss -374.1841, train acc 1.0000\n",
      "[Epoch 112/200] Loss -377.5439, train acc 1.0000\n",
      "[Epoch 113/200] Loss -380.9036, train acc 1.0000\n",
      "[Epoch 114/200] Loss -384.2632, train acc 1.0000\n",
      "[Epoch 115/200] Loss -387.6224, train acc 1.0000\n",
      "[Epoch 116/200] Loss -390.9816, train acc 1.0000\n",
      "[Epoch 117/200] Loss -394.3403, train acc 1.0000\n",
      "[Epoch 118/200] Loss -397.6989, train acc 1.0000\n",
      "[Epoch 119/200] Loss -401.0575, train acc 1.0000\n",
      "Accuracy: 0.6160\n",
      "[Epoch 120/200] Loss -404.4157, train acc 1.0000\n",
      "[Epoch 121/200] Loss -407.7737, train acc 1.0000\n",
      "[Epoch 122/200] Loss -411.1314, train acc 1.0000\n",
      "[Epoch 123/200] Loss -414.4888, train acc 1.0000\n",
      "[Epoch 124/200] Loss -417.8464, train acc 1.0000\n",
      "[Epoch 125/200] Loss -421.2036, train acc 1.0000\n",
      "[Epoch 126/200] Loss -424.5604, train acc 1.0000\n",
      "[Epoch 127/200] Loss -427.9172, train acc 1.0000\n",
      "[Epoch 128/200] Loss -431.2738, train acc 1.0000\n",
      "[Epoch 129/200] Loss -434.6302, train acc 1.0000\n",
      "Accuracy: 0.6170\n",
      "[Epoch 130/200] Loss -437.9863, train acc 1.0000\n",
      "[Epoch 131/200] Loss -441.3422, train acc 1.0000\n",
      "[Epoch 132/200] Loss -444.6981, train acc 1.0000\n",
      "[Epoch 133/200] Loss -448.0533, train acc 1.0000\n",
      "[Epoch 134/200] Loss -451.4087, train acc 1.0000\n",
      "[Epoch 135/200] Loss -454.7639, train acc 1.0000\n",
      "[Epoch 136/200] Loss -458.1187, train acc 1.0000\n",
      "[Epoch 137/200] Loss -461.4734, train acc 1.0000\n",
      "[Epoch 138/200] Loss -464.8278, train acc 1.0000\n",
      "[Epoch 139/200] Loss -468.1821, train acc 1.0000\n",
      "Accuracy: 0.6160\n",
      "[Epoch 140/200] Loss -471.5363, train acc 1.0000\n",
      "[Epoch 141/200] Loss -474.8901, train acc 1.0000\n",
      "[Epoch 142/200] Loss -478.2437, train acc 1.0000\n",
      "[Epoch 143/200] Loss -481.5972, train acc 1.0000\n",
      "[Epoch 144/200] Loss -484.9504, train acc 1.0000\n",
      "[Epoch 145/200] Loss -488.3032, train acc 1.0000\n",
      "[Epoch 146/200] Loss -491.6562, train acc 1.0000\n",
      "[Epoch 147/200] Loss -495.0087, train acc 1.0000\n",
      "[Epoch 148/200] Loss -498.3611, train acc 1.0000\n",
      "[Epoch 149/200] Loss -501.7133, train acc 1.0000\n",
      "Accuracy: 0.6150\n",
      "[Epoch 150/200] Loss -505.0653, train acc 1.0000\n",
      "[Epoch 151/200] Loss -508.4170, train acc 1.0000\n",
      "[Epoch 152/200] Loss -511.7685, train acc 1.0000\n",
      "[Epoch 153/200] Loss -515.1199, train acc 1.0000\n",
      "[Epoch 154/200] Loss -518.4710, train acc 1.0000\n",
      "[Epoch 155/200] Loss -521.8219, train acc 1.0000\n",
      "[Epoch 156/200] Loss -525.1727, train acc 1.0000\n",
      "[Epoch 157/200] Loss -528.5232, train acc 1.0000\n",
      "[Epoch 158/200] Loss -531.8735, train acc 1.0000\n",
      "[Epoch 159/200] Loss -535.2237, train acc 1.0000\n",
      "Accuracy: 0.6100\n",
      "[Epoch 160/200] Loss -538.5734, train acc 1.0000\n",
      "[Epoch 161/200] Loss -541.9233, train acc 1.0000\n",
      "[Epoch 162/200] Loss -545.2726, train acc 1.0000\n",
      "[Epoch 163/200] Loss -548.6218, train acc 1.0000\n",
      "[Epoch 164/200] Loss -551.9711, train acc 1.0000\n",
      "[Epoch 165/200] Loss -555.3200, train acc 1.0000\n",
      "[Epoch 166/200] Loss -558.6686, train acc 1.0000\n",
      "[Epoch 167/200] Loss -562.0169, train acc 1.0000\n",
      "[Epoch 168/200] Loss -565.3653, train acc 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 169/200] Loss -568.7132, train acc 1.0000\n",
      "Accuracy: 0.6110\n",
      "[Epoch 170/200] Loss -572.0609, train acc 1.0000\n",
      "[Epoch 171/200] Loss -575.4086, train acc 1.0000\n",
      "[Epoch 172/200] Loss -578.7562, train acc 1.0000\n",
      "[Epoch 173/200] Loss -582.1032, train acc 1.0000\n",
      "[Epoch 174/200] Loss -585.4502, train acc 1.0000\n",
      "[Epoch 175/200] Loss -588.7969, train acc 1.0000\n",
      "[Epoch 176/200] Loss -592.1436, train acc 1.0000\n",
      "[Epoch 177/200] Loss -595.4899, train acc 1.0000\n",
      "[Epoch 178/200] Loss -598.8361, train acc 1.0000\n",
      "[Epoch 179/200] Loss -602.1819, train acc 1.0000\n",
      "Accuracy: 0.6080\n",
      "[Epoch 180/200] Loss -605.5276, train acc 1.0000\n",
      "[Epoch 181/200] Loss -608.8732, train acc 1.0000\n",
      "[Epoch 182/200] Loss -612.2187, train acc 1.0000\n",
      "[Epoch 183/200] Loss -615.5637, train acc 1.0000\n",
      "[Epoch 184/200] Loss -618.9085, train acc 1.0000\n",
      "[Epoch 185/200] Loss -622.2532, train acc 1.0000\n",
      "[Epoch 186/200] Loss -625.5977, train acc 1.0000\n",
      "[Epoch 187/200] Loss -628.9423, train acc 1.0000\n",
      "[Epoch 188/200] Loss -632.2861, train acc 1.0000\n",
      "[Epoch 189/200] Loss -635.6302, train acc 1.0000\n",
      "Accuracy: 0.6070\n",
      "[Epoch 190/200] Loss -638.9738, train acc 1.0000\n",
      "[Epoch 191/200] Loss -642.3173, train acc 1.0000\n",
      "[Epoch 192/200] Loss -645.6605, train acc 1.0000\n",
      "[Epoch 193/200] Loss -649.0036, train acc 1.0000\n",
      "[Epoch 194/200] Loss -652.3463, train acc 1.0000\n",
      "[Epoch 195/200] Loss -655.6890, train acc 1.0000\n",
      "[Epoch 196/200] Loss -659.0312, train acc 1.0000\n",
      "[Epoch 197/200] Loss -662.3736, train acc 1.0000\n",
      "[Epoch 198/200] Loss -665.7156, train acc 1.0000\n",
      "[Epoch 199/200] Loss -669.0573, train acc 1.0000\n",
      "Accuracy: 0.6060\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(task2gnn.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "for epoch in range(200):\n",
    "    task2gnn.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get output\n",
    "    out = task2gnn(data.x, data.edge_index)\n",
    "    \n",
    "    # Get loss\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    _, pred = out.max(dim=1)\n",
    "    \n",
    "    # Get predictions and calculate training accuracy\n",
    "    correct = float(pred[data.train_mask].eq(data.y[data.train_mask]).sum().item())\n",
    "    acc = correct / data.train_mask.sum().item()\n",
    "    print('[Epoch {}/200] Loss {:.4f}, train acc {:.4f}'.format(epoch, loss.cpu().detach().data.item(), acc))\n",
    "    \n",
    "    # Backward\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Evaluation on test data every 10 epochs\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        task2gnn.eval()\n",
    "        _, pred = task2gnn(data.x, data.edge_index).max(dim=1)\n",
    "        correct = float(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "        acc = correct / data.test_mask.sum().item()\n",
    "        print('Accuracy: {:.4f}'.format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
