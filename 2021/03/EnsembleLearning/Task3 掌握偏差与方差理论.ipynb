{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**写在前面**：本节内容是 [Datawhale三月的组队学习 - 集成学习（上）- CH2-机器学习基础模型回顾 -【Task3 掌握偏差与方差理论】](https://github.com/datawhalechina/team-learning-data-mining/blob/master/EnsembleLearning/CH2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%E5%9B%9E%E9%A1%BE/%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80.ipynb) 的学习笔记，对应notebook的2.1(4)节，学习周期2天"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入库和数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T05:05:59.127576Z",
     "start_time": "2021-03-21T05:05:51.078030Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection  import train_test_split\n",
    "import xgboost\n",
    "# Day1\n",
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "from sklearn.linear_model import LassoLarsIC\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "# Day2\n",
    "from sklearn.model_selection import KFold # K折交叉验证\n",
    "from sklearn.model_selection import LeaveOneOut # 留一法\n",
    "from sklearn.model_selection import StratifiedKFold # 分层交叉验证\n",
    "from sklearn.model_selection import LeavePOut # 留P交叉验证\n",
    "from sklearn.model_selection import GroupKFold # 分组交叉验证\n",
    "import random\n",
    "from sklearn.linear_model import Lasso,Ridge,LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "# 测试用\n",
    "import numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T05:05:59.174557Z",
     "start_time": "2021-03-21T05:05:59.129572Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# boston数据集作为本笔记的回归实验数据\n",
    "boston = datasets.load_boston()\n",
    "Xb = boston.data\n",
    "yb = boston.target\n",
    "features = boston.feature_names\n",
    "boston_data = pd.DataFrame(Xb,columns=features)\n",
    "boston_data[\"Price\"] = yb\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xb,yb,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T05:05:59.205172Z",
     "start_time": "2021-03-21T05:05:59.176543Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# breast_cancer数据集作为本笔记的分类实验数据\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "Xbc = breast_cancer.data\n",
    "ybc = breast_cancer.target\n",
    "features = breast_cancer.feature_names\n",
    "breast_cancer = pd.DataFrame(Xbc,columns=features)\n",
    "breast_cancer[\"target\"] = ybc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T05:05:59.471455Z",
     "start_time": "2021-03-21T05:05:59.207160Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:\t20.43824\n"
     ]
    }
   ],
   "source": [
    "# 使用xgboost的回归模型作为实验模型\n",
    "model = xgboost.XGBRegressor()\n",
    "model.fit(X=X_train, y=y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print('MSE:\\t%.5f' % MSE(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 概念理解\n",
    "**参考资料**：\n",
    "- [偏差和方差有什么区别？](https://www.zhihu.com/question/20448464/answer/765401873)\n",
    "- 微信公众号:数据派THU —《干货 ：教你用Python来计算偏差-方差权衡》\n",
    "- [理解赤池信息量（AIC）,贝叶斯信息量（BIC）](https://blog.csdn.net/chieryu/article/details/51746554)\n",
    "- [模型选择方法：AIC和BIC](https://www.jianshu.com/p/4c8cf5df2092)\n",
    "- [AIC, BIC 和 L1,L2 等正则化有什么区别？](https://zhuanlan.zhihu.com/p/26372789)\n",
    "- [误差与残差](https://blog.csdn.net/fwj_ntu/article/details/82697433)\n",
    "- [残差、方差、偏差、MSE均方误差、Bagging、Boosting、过拟合欠拟合和交叉验证](https://blog.csdn.net/u010986753/article/details/102495494)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **偏差**：bias  \n",
    "    指**预测结果**与**真实值**之间的`差异`，排除噪声的影响。反映模型本身的精确度。\n",
    "        偏差更多的是针对某个模型输出的样本误差，偏差是模型无法准确表达数据关系导致，比如模型过于简单，非线性的数据关系采用线性模型建模，偏差较大的模型是错的模型\n",
    "\n",
    "\n",
    "- **方差**：variance  \n",
    "    指多个(次)模型**输出的结果**之间的`离散差异`。反映模型的稳定性。\n",
    "        注意这里写的是多个模型或者多次模型，即不同模型或同一模型不同时间的输出结果方差较大，方差是由训练集的数据不够导致。一方面量 【数据量】不够，有限的数据集过度训练导致模型复杂；另一方面质【样本质量】不行，测试集中的数据分布未在训练集中，导致每次抽样训练模型时，每次模型参数不同，输出的结果都无法准确的预测出正确结果  \n",
    "\n",
    "\n",
    "- **偏差-方差分解**  \n",
    "可证得：(我还没证)\n",
    "   $$\n",
    "   E\\left(y_{0}-\\hat{f}\\left(x_{0}\\right)\\right)^{2}=\\operatorname{Var}\\left(\\hat{f}\\left(x_{0}\\right)\\right)+\\left[\\operatorname{Bias}\\left(\\hat{f}\\left(x_{0}\\right)\\right)\\right]^{2}+\\operatorname{Var}(\\varepsilon)\n",
    "   $$      \n",
    "\n",
    "\n",
    "- **误差**：Errors  \n",
    "    指**观测值**与**真实值**的偏差。  \n",
    "\n",
    "\n",
    "- **残差**：Residuals  \n",
    "    指**估计值**与**观测值**的偏差。如果回归模型正确的话， 我们可以将残差看作误差的观测值。  \n",
    "\n",
    "\n",
    "- **残差、方差、偏差总结**\n",
    "    - **简单模型**：偏差大，方差小（简单模型受样本值的影响较小，稳定性高），容易造成欠拟合\n",
    "    - **复杂模型**：偏差小，方差大，容易产生过拟合\n",
    "    - 判断偏差大还是方差大：\n",
    "        1. 模型上的训练样本的真实值较少，则偏差大（欠拟合）\n",
    "        2. 在训练样本上样本拟合的较好，但在测试集上拟合较差，则方差大（过拟合Overfiting）\n",
    "        3. 当偏差较大时，表示目标可能未在模型上（即未瞄准靶心），需要重新训练model（有可能未考虑其他因素对样本的影响，或者应让模型更复杂考虑更高次幂的情况）。增加网络层数，增加隐藏层神经元数量，增加算法迭代次数，或者用更好的优化算法。\n",
    "        4. 当方差较大时：增加更多的数据或正则化  \n",
    "        \n",
    "- **AIC赤池信息量**  \n",
    "越小越好。也写做：-2ln(L) + 2k\n",
    "$$AIC = \\frac{1}{d\\hat{\\sigma}^2}(RSS  +  2d\\hat{\\sigma}^2)$$      \n",
    "- **BIC贝叶斯信息量**  \n",
    "也写做：-2ln(L) + ln(n)*k\n",
    "$$BIC = \\frac{1}{n}(RSS + log(n)d\\hat{\\sigma}^2)$$    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 偏差-方差分解mlxtend调用实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T15:03:16.098972Z",
     "start_time": "2021-03-19T15:02:49.378859Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:\t15.416\n",
      "Bias:\t10.676\n",
      "Var:\t4.740\n"
     ]
    }
   ],
   "source": [
    "mse,bias,var = bias_variance_decomp(model, X_train, y_train, X_test, y_test,loss='mse', num_rounds=200)\n",
    "print('MSE:\\t%.3f' % mse)\n",
    "print('Bias:\\t%.3f' % bias)\n",
    "print('Var:\\t%.3f' % var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现这里`MSE = Bias + Var`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练误差修正\n",
    "**参考资料**:\n",
    "- [aic python_使用python+sklearn实现Lasso 模型选择：交叉验证/ AIC / BIC](https://blog.csdn.net/weixin_35473090/article/details/112500913)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AIC赤池信息量准则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T15:43:00.906169Z",
     "start_time": "2021-03-19T15:43:00.884226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIC:\t0.00143\n",
      "MSE:\t31.18560\n"
     ]
    }
   ],
   "source": [
    "model_aic = LassoLarsIC(criterion='aic')\n",
    "model_aic.fit(X_train, y_train)\n",
    "y_pred = model_aic.predict(X_test)\n",
    "print('AIC:\\t%.5f' % model_aic.alpha_)\n",
    "print('MSE:\\t%.5f' % MSE(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BIC贝叶斯信息量准则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T15:43:02.609107Z",
     "start_time": "2021-03-19T15:43:02.591155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIC:\t0.07306\n",
      "MSE:\t39.80559\n"
     ]
    }
   ],
   "source": [
    "model_bic = LassoLarsIC(criterion='bic')\n",
    "model_bic.fit(X_train, y_train)\n",
    "y_pred = model_bic.predict(X_test)\n",
    "print('BIC:\\t%.5f' % model_bic.alpha_)\n",
    "print('MSE:\\t%.5f' % MSE(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 交叉验证：Cross Validation\n",
    "**参考资料**：\n",
    "- [数据集划分train_test_split\\交叉验证Cross-validation](https://blog.csdn.net/u010986753/article/details/98069124)\n",
    "- [sklearn.model_selection](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection)\n",
    "- 《机器学习》 - 周志华"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 简单交叉验证：train_test_split\n",
    "通过反复重新选择训练集和测试集，继续训练数据和检验模型，最后选择损失函数评估最优的模型和参数。\n",
    "- **好处**：  \n",
    "    处理简单，只需随机把原始数据分为两组即可。\n",
    "    \n",
    "- **缺点**：  \n",
    "    只进行了一次划分，数据结果具有偶然性，没有达到交叉的思想，由于是随机的将原始数据分组，所以最后验证集分类准确率的高低与原始数据的分组有很大的关系，得到的结果并不具有说服性。\n",
    "    \n",
    "**sklearn参数**：  \n",
    "- `test_size`：测试集的样本比例或样本数量\n",
    "- `shuffle`：拆分前是否对数据进行混洗\n",
    "\n",
    "**注**：参数未全部列出，详细可见参考文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T04:05:54.494363Z",
     "start_time": "2021-03-20T04:05:54.473419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.length: 568\t, \ttest.length: 1\n",
      "train==0: 147\t, train=1: 251\t, ratio: 0.59\n",
      "test==0:  65\t, test=1:  106\t, ratio: 0.61\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Xbc, ybc, test_size=0.30,\n",
    "                                                    shuffle=True, random_state=320)\n",
    "a, b = len(y_train[y_train == 0]), len(y_train[y_train == 1])\n",
    "c, d = len(y_test[y_test == 0]), len(y_test[y_test == 1])\n",
    "print('train.length: %d\\t, \\ttest.length: %d' %\n",
    "      (train_index.shape[0], test_index.shape[0]))\n",
    "print('train==0: %d\\t, train=1: %d\\t, ratio: %.2f' % (a, b, a / b))\n",
    "print('test==0:  %d\\t, test=1:  %d\\t, ratio: %.2f' % (c, d, c / d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 留一交叉验证 Leave-one-out Cross Validation\n",
    "在数据缺乏的情况下使用，如果设原始数据有N个样本，那么LOO-CV就是N-CV，即每个样本单独作为验证集，其余的N-1个样本作为训练集，故LOO-CV会得到N个模型，用这N个模型最终的验证集的分类准确率的平均数作为此下LOO-CV分类器的性能指标。\n",
    "- **优点**：  \n",
    "    不存在数据分布不一致,每一回合中几乎所有的样本皆用于训练模型，因此最接近原始样本的分布，这样评估所得的结果比较可靠。实验过程中没有随机因素会影响实验数据，确保实验过程是可以被复制的。\n",
    "- **缺点**：  \n",
    "    耗时,计算成本高，需要建立的模型数量与原始数据样本数量相同。当数据集较大时几乎不能使用。\n",
    "\n",
    "**sklearn参数**：  \n",
    "- 无"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T04:02:09.582680Z",
     "start_time": "2021-03-20T04:02:09.549714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop: 568\n"
     ]
    }
   ],
   "source": [
    "LOO = LeaveOneOut()\n",
    "for loop, (train_idx, test_idx) in enumerate(LOO.split(Xbc, ybc)):\n",
    "    X_train, X_test = Xbc[train_idx], Xbc[test_idx]\n",
    "    y_train, y_test = ybc[train_idx], ybc[test_idx]\n",
    "#     a,b = len(y_train[y_train==0]),len(y_train[y_train==1])\n",
    "#     c,d = len(y_test[y_test==0]),len(y_test[y_test==1])\n",
    "#     print('################(%d)################' % loop)\n",
    "#     print('train.length: %d, test.length: %d' % (train_idx.shape[0], test_idx.shape[0]))\n",
    "#     print('train==0: %d\\t, train=1: %d\\t, ratio: %.2f' % (a,b,a/b))\n",
    "#     print('test==0:  %d\\t, test=1:  %d\\t, ratio: %.2f' % (c,d,c/d))\n",
    "else:\n",
    "    print('loop:',loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 留P交叉验证 LeavePOut\n",
    "留一法的变体。它从完整的数据集里删除 p 个样本，产生所有可能的训练集和检验集。对于 n个样本，能产生m个训练-检验对。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T04:12:13.017188Z",
     "start_time": "2021-03-20T04:12:12.442317Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50077ba43c964c8fa3717443f98fc960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LPO= LeavePOut(p=300)\n",
    "for loop, (train_idx, test_idx) in tqdm(enumerate(LPO.split(Xbc, ybc))):\n",
    "    if loop>10000:\n",
    "        break\n",
    "    X_train, X_test = Xbc[train_idx], Xbc[test_idx]\n",
    "    y_train, y_test = ybc[train_idx], ybc[test_idx]\n",
    "#     a,b = len(y_train[y_train==0]),len(y_train[y_train==1])\n",
    "#     c,d = len(y_test[y_test==0]),len(y_test[y_test==1])\n",
    "#     print('################(%d)################' % loop)\n",
    "#     print('train.length: %d, test.length: %d' % (train_idx.shape[0], test_idx.shape[0]))\n",
    "#     print('train==0: %d\\t, train=1: %d\\t, ratio: %.2f' % (a,b,a/b))\n",
    "#     print('test==0:  %d\\t, test=1:  %d\\t, ratio: %.2f' % (c,d,c/d))\n",
    "else:\n",
    "    print('loop:',loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 自助法 Bootstrapping\n",
    "另一种比较特殊的交叉验证方式，也用于样本量少的时候。假设有m个样本（m较小），每次在这m个样本中随机采集一个样本，放入训练集，采样完后把样本放回。这样重复采集m次，我们得到m个样本组成的训练集。这m个样本中很有可能有重复的样本数据。同时，用没有被采样到的样本做测试集。这样接着进行交叉验证。由于我们的训练集有重复数据，这会改变数据的分布，因而训练结果会有估计偏差，因此，此种方法不是很常用，除非数据量真的很少，比如小于20个。\n",
    "- **优点**：  \n",
    "在数据集较小、难以划分时很有用，能从D中产生不同的S，对集成学习等方法有好处\n",
    "\n",
    "- **缺点**：  \n",
    "产生的S改变了D的分布，会引入估计偏差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T06:46:58.770257Z",
     "start_time": "2021-03-20T06:46:58.447509Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(Xb)\n",
    "target = pd.DataFrame(yb)\n",
    "for loop in range(data.shape[0]):\n",
    "    X_train = data.sample(frac=1.0,replace=True) # 有放回随机采样\n",
    "    y_train = target.sample(frac=1.0,replace=True) \n",
    "    X_test = data.loc[X_train.index.difference(X_train.index)].copy() # 将未采样的样本作为测试集\n",
    "    y_test = target.loc[y_train.index.difference(y_train.index)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K折交叉验证 K-Folder Cross Validation\n",
    "应用最多，K-CV可以有效的避免过拟合与欠拟合的发生，最后得到的结果也比较具有说服性。\n",
    "- **实现步骤**：\n",
    "    1. 不重复抽样将原始数据随机分为 k 份。\n",
    "    2. 每一次挑选其中 1 份作为测试集，剩余 k-1 份作为训练集用于模型训练。\n",
    "    3. 重复(2) k 次，这样每个子集都有一次机会作为测试集，其余机会作为训练集。在每个训练集上训练后得到一个模型，用这个模型在相应的测试集上测试，计算并保存模型的评估指标，\n",
    "    4. 计算 k 组测试结果的平均值作为模型精度的估计，并作为当前 k 折交叉验证下模型的性能指标。\n",
    "  \n",
    "  \n",
    "- **优点**：  \n",
    "    降低由一次随机划分带来的偶然性，提高其泛化能力，提高对数据的使用效率。\n",
    "- **缺点**：  \n",
    "    可能存在一种情况：数据集有5类，抽取出来的也正好是按照类别划分的5类，也就是说第一折全是0类，第二折全是1类，等等；这样的结果就会导致，模型训练时。没有学习到测试集中数据的特点，从而导致模型得分很低，甚至为0\n",
    "    \n",
    "**sklearn参数**：  \n",
    "- `n_splits`：折数\n",
    "- `shuffle`：拆分前是否对数据进行混洗\n",
    "\n",
    "**注**：参数未全部列出，详细可见参考文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T09:04:43.274390Z",
     "start_time": "2021-03-20T09:04:43.253550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################(0)######################\n",
      "train.length: 379\t, \ttest.length: 190\n",
      "train==0: 139\t, train=1: 240\t, ratio: 0.58\n",
      "test==0:  73\t, test=1:  117\t, ratio: 0.62\n",
      "######################(1)######################\n",
      "train.length: 379\t, \ttest.length: 190\n",
      "train==0: 135\t, train=1: 244\t, ratio: 0.55\n",
      "test==0:  77\t, test=1:  113\t, ratio: 0.68\n",
      "######################(2)######################\n",
      "train.length: 380\t, \ttest.length: 189\n",
      "train==0: 150\t, train=1: 230\t, ratio: 0.65\n",
      "test==0:  62\t, test=1:  127\t, ratio: 0.49\n",
      "loop: 2\n"
     ]
    }
   ],
   "source": [
    "KF = KFold(n_splits=3, shuffle=True, random_state=320)\n",
    "for loop, (train_idx, test_idx) in enumerate(KF.split(Xbc, ybc)):\n",
    "    if loop>10000:\n",
    "        break\n",
    "    X_train, X_test = Xbc[train_idx], Xbc[test_idx]\n",
    "    y_train, y_test = ybc[train_idx], ybc[test_idx]\n",
    "    a,b = len(y_train[y_train==0]),len(y_train[y_train==1])\n",
    "    c,d = len(y_test[y_test==0]),len(y_test[y_test==1])\n",
    "    print('######################(%d)######################' % loop)\n",
    "    print('train.length: %d\\t, \\ttest.length: %d' % (train_idx.shape[0], test_idx.shape[0]))\n",
    "    print('train==0: %d\\t, train=1: %d\\t, ratio: %.2f' % (a,b,a/b))\n",
    "    print('test==0:  %d\\t, test=1:  %d\\t, ratio: %.2f' % (c,d,c/d))\n",
    "else:\n",
    "    print('loop:',loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 分层交叉验证 StratifiedKFold\n",
    "KFold的变体。对非平衡数据可以用分层采样，能够在每一份子集中都保持和原始数据集相同的类别比例。  \n",
    "\n",
    "**sklearn参数**：  \n",
    "- `n_splits`：折数\n",
    "- `shuffle`：拆分前是否对数据进行混洗\n",
    "\n",
    "**注**：参数未全部列出，详细可见参考文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T04:13:24.021299Z",
     "start_time": "2021-03-20T04:13:24.007556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################(0)######################\n",
      "train.length: 379\t, \ttest.length: 190\n",
      "train==0: 141\t, train=1: 238\t, ratio: 0.59\n",
      "test==0:  71\t, test=1:  119\t, ratio: 0.60\n",
      "######################(1)######################\n",
      "train.length: 379\t, \ttest.length: 190\n",
      "train==0: 141\t, train=1: 238\t, ratio: 0.59\n",
      "test==0:  71\t, test=1:  119\t, ratio: 0.60\n",
      "######################(2)######################\n",
      "train.length: 380\t, \ttest.length: 189\n",
      "train==0: 142\t, train=1: 238\t, ratio: 0.60\n",
      "test==0:  70\t, test=1:  119\t, ratio: 0.59\n",
      "loop: 2\n"
     ]
    }
   ],
   "source": [
    "SKF = StratifiedKFold(n_splits=3, shuffle=True, random_state=320)\n",
    "for loop, (train_idx, test_idx) in enumerate(SKF.split(Xbc, ybc)):\n",
    "    X_train, X_test = Xbc[train_idx], Xbc[test_idx]\n",
    "    y_train, y_test = ybc[train_idx], ybc[test_idx]\n",
    "    a,b = len(y_train[y_train==0]),len(y_train[y_train==1])\n",
    "    c,d = len(y_test[y_test==0]),len(y_test[y_test==1])\n",
    "    print('######################(%d)######################' % loop)\n",
    "    print('train.length: %d\\t, \\ttest.length: %d' % (train_idx.shape[0], test_idx.shape[0]))\n",
    "    print('train==0: %d\\t, train=1: %d\\t, ratio: %.2f' % (a,b,a/b))\n",
    "    print('test==0:  %d\\t, test=1:  %d\\t, ratio: %.2f' % (c,d,c/d))\n",
    "else:\n",
    "    print('loop:',loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 分组交叉验证 GroupKFold\n",
    "KFold的变体，确保有一个 group 在测试和训练集中都不被表示。通过留出一组特定的不属于测试集和训练集的数据，来测试训练的模型在未知 group 上的性能。需要设定组。\n",
    "\n",
    "**sklearn参数**：  \n",
    "- `n_splits`：折数\n",
    "- `shuffle`：拆分前是否对数据进行混洗\n",
    "\n",
    "**注**：参数未全部列出，详细可见参考文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T09:04:59.095570Z",
     "start_time": "2021-03-20T09:04:59.077540Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################(0)######################\n",
      "train.length: 367\t, \ttest.length: 202\n",
      "train==0: 137\t, train=1: 230\t, ratio: 0.60\n",
      "test==0:  75\t, test=1:  127\t, ratio: 0.59\n",
      "######################(1)######################\n",
      "train.length: 384\t, \ttest.length: 185\n",
      "train==0: 143\t, train=1: 241\t, ratio: 0.59\n",
      "test==0:  69\t, test=1:  116\t, ratio: 0.59\n",
      "######################(2)######################\n",
      "train.length: 387\t, \ttest.length: 182\n",
      "train==0: 144\t, train=1: 243\t, ratio: 0.59\n",
      "test==0:  68\t, test=1:  114\t, ratio: 0.60\n",
      "loop: 3\n"
     ]
    }
   ],
   "source": [
    "GKF = GroupKFold(n_splits=3)\n",
    "groups = np.array([random.randint(0,2) for i in range(Xbc.shape[0])])\n",
    "for loop, (train_idx, test_idx) in enumerate(GKF.split(Xbc, ybc, groups)):\n",
    "    if loop>10000:\n",
    "        break\n",
    "    X_train, X_test = Xbc[train_idx], Xbc[test_idx]\n",
    "    y_train, y_test = ybc[train_idx], ybc[test_idx]\n",
    "    a,b = len(y_train[y_train==0]),len(y_train[y_train==1])\n",
    "    c,d = len(y_test[y_test==0]),len(y_test[y_test==1])\n",
    "    print('######################(%d)######################' % loop)\n",
    "    print('train.length: %d\\t, \\ttest.length: %d' % (train_idx.shape[0], test_idx.shape[0]))\n",
    "    print('train==0: %d\\t, train=1: %d\\t, ratio: %.2f' % (a,b,a/b))\n",
    "    print('test==0:  %d\\t, test=1:  %d\\t, ratio: %.2f' % (c,d,c/d))\n",
    "else:\n",
    "    print('loop:',loop+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 特征选择：Feature Selection\n",
    "**参考资料**：\n",
    "- [机器学习：特征选择（feature selection）](https://blog.csdn.net/qq_33876194/article/details/88403394?utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.control&dist_request_id=&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 最优子集选择\n",
    "子集的数量为$2^p$，计算效率低下且需要很高的计算内存\n",
    "- **步骤**：\n",
    "    1. 记不含任何特征的模型为$M_0$，计算这个$M_0$的`测试误差`。                              \n",
    "    2. 在$M_0$基础上增加一个变量，计算p个模型的RSS，选择RSS最小的模型记作$M_1$，并计算该模型$M_1$的`测试误差`。\n",
    "    3. 再增加变量，计算p-1个模型的RSS，并选择RSS最小的模型记作$M_2$，并计算该模型$M_2$的`测试误差`。       \n",
    "    4. 重复以上过程知道拟合的模型有p个特征为止，并选择p+1个模型$\\{M_0,M_1,...,M_p \\}$中`测试误差`最小的模型作为最优模型。\n",
    "    \n",
    "  **注**：`测试误差`值测试集上的误差，指标自选，如：MSE等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感觉这个方法写起来复杂，运算起来资源也消耗的大，不是很好。不准备写了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T08:40:48.396231Z",
     "start_time": "2021-03-20T08:40:48.391240Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample_select_best_subset(X:np.array, y:np.array, error:'func'=MSE, random_stat=None):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 向前逐步选择\n",
    "最优子集选择的优化\n",
    "- **步骤**：\n",
    "    1. 记不含任何特征的模型为$M_0$，计算这个$M_0$的测试误差。                    \n",
    "    2. 在$M_0$基础上增加一个变量，计算p个模型的RSS，选择RSS最小的模型记作$M_1$，并计算该模型$M_1$的测试误差。   \n",
    "    3. 在最小的RSS模型下继续增加一个变量，选择RSS最小的模型记作$M_2$，并计算该模型$M_2$的测试误差。             \n",
    "    4. 以此类推，重复以上过程知道拟合的模型有p个特征为止，并选择p+1个模型$\\{M_0,M_1,...,M_p \\}$中测试误差最小的模型作为最优模型。   \n",
    "        \n",
    "  **注**：`测试误差`指测试集上的误差，指标自选，如：MSE等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T05:46:22.812355Z",
     "start_time": "2021-03-21T05:46:22.801386Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample_forward_select(X, y, error_func:'func'=MSE, model:'func'=Lasso, random_state=None):\n",
    "    variate_lst = list(map(str,range(X.shape[1]))) # 将位置作为特征名\n",
    "    best_idx_lst = [] # \n",
    "    best_error_lst = []\n",
    "    best_model_lst = []\n",
    "    model = model()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,\n",
    "                                                        random_state=random_state)\n",
    "    while variate_lst: # 当还有特征时继续进行\n",
    "        best_error = float('inf')\n",
    "        for variate in variate_lst: # 从剩下的特征里按顺序选特征\n",
    "            select_idx = best_idx_lst + [int(variate)] # 当前的特征索引序列\n",
    "            select_data, select_target = X_train[:, select_idx], X_test[:, select_idx] \n",
    "            y_pred = model.fit(select_data, y_train).predict(select_target)\n",
    "            error = error_func(y_test, y_pred) # 得到测试误差\n",
    "            if error < best_error:\n",
    "                best_error = error\n",
    "                best_idx = int(variate)\n",
    "        else:\n",
    "            best_idx_lst.append(best_idx)\n",
    "            best_error_lst.append(best_error)\n",
    "            best_model_lst.append(model)\n",
    "            variate_lst.remove(str(best_idx))\n",
    "    else:\n",
    "        # 这里写的比较复杂。实际上就是得到最小error，同时得到最小error对应的model\n",
    "        best_error,best_model = sorted(list(zip(best_error_lst,best_model_lst)),key=lambda x:x[0])[0]\n",
    "        topk = best_error_lst.index(best_error) # 找到选取几个特征合适\n",
    "        best_select_variate = best_idx_lst[:topk] # 选取topk个最合适的特征\n",
    "        return best_select_variate,best_error,best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T05:46:03.770421Z",
     "start_time": "2021-03-21T05:46:03.698742Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best features:\t LSTAT | PTRATIO | RM | CRIM | RAD | INDUS | DIS | ZN\n",
      "best error:\t 17.22996\n"
     ]
    }
   ],
   "source": [
    "# model:Lasso, error:MSE\n",
    "best_select_variate, best_error, best_model = sample_forward_select(Xb,yb,random_state=320)\n",
    "print('best features:\\t',' | '.join(boston_data.columns[best_select_variate]))\n",
    "print('best error:\\t %.5f' % best_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T08:59:59.351230Z",
     "start_time": "2021-03-20T08:59:59.297389Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best features:\t LSTAT | PTRATIO | RM | DIS | NOX | ZN | RAD | TAX\n",
      "best error:\t 17.91369\n"
     ]
    }
   ],
   "source": [
    "# model:Ridge, error:MSE\n",
    "best_select_variate, best_error, best_model = sample_forward_select(Xb,yb,random_state=320,\n",
    "                                                                   model=Ridge, error_func=MSE)\n",
    "print('best features:\\t',' | '.join(boston_data.columns[best_select_variate]))\n",
    "print('best error:\\t %.5f' % best_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T09:00:05.012028Z",
     "start_time": "2021-03-20T09:00:04.943425Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best features:\t LSTAT | PTRATIO | RM | B | RAD | CHAS | NOX | INDUS | ZN | DIS\n",
      "best error:\t 3.07599\n"
     ]
    }
   ],
   "source": [
    "# model:Lasso, error:MAE\n",
    "best_select_variate, best_error, best_model = sample_forward_select(Xb,yb,random_state=320,\n",
    "                                                                   model=Lasso, error_func=MAE)\n",
    "print('best features:\\t',' | '.join(boston_data.columns[best_select_variate]))\n",
    "print('best error:\\t %.5f' % best_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T09:00:07.913606Z",
     "start_time": "2021-03-20T09:00:07.847157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best features:\t LSTAT | PTRATIO | RM | B | CRIM | TAX | NOX | DIS | INDUS\n",
      "best error:\t 3.05965\n"
     ]
    }
   ],
   "source": [
    "# model:Ridge, error:MAE\n",
    "best_select_variate, best_error, best_model = sample_forward_select(Xb,yb,random_state=320,\n",
    "                                                                   model=Ridge, error_func=MAE)\n",
    "print('best features:\\t',' | '.join(boston_data.columns[best_select_variate]))\n",
    "print('best error:\\t %.5f' % best_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 压缩估计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 岭回归(L2)\n",
    "![./Image/Ridge.svg](./Image/Ridge.svg)\n",
    "**参考资料**：\n",
    "- [sklearn 中的线性回归、岭回归、Lasso回归参数配置及示例](https://blog.csdn.net/VariableX/article/details/107166602)\n",
    "- [用scikit-learn和pandas学习Ridge回归](https://www.cnblogs.com/pinard/p/6023000.html)\n",
    "- [从Lasso开始说起](https://zhuanlan.zhihu.com/p/46999826)\n",
    "- [手写算法-python代码实现Ridge(L2正则项)回归](https://blog.csdn.net/weixin_44700798/article/details/110738525)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sklearn调用\n",
    "**sklearn参数**：\n",
    "- `alpha`: 正则项系数。数值越大，则对复杂模型的惩罚力度越大。\n",
    "        调参方法：\n",
    "        1. 给定alpha较小的值，例如0.1。\n",
    "        2. 根据验证集准确率以10倍为单位增大或者减小参数值。\n",
    "        3. 在找到合适的数量级后，在此数量级上微调。\n",
    "        合适的候选值：[0.001, 0.01, 0.1, 1, 10, 100]\n",
    "- `normalize`: 是否对各个特征进行标准化。（默认方式：减去均值并除以l2范数）\n",
    "        标准化的好处:\n",
    "        1. 加速收敛\n",
    "        2. 提升精度\n",
    "- `fit_intercept`: 是否计算截距。\n",
    "- `solver`: 解决优化问题的算法\n",
    "    - `svd`: 采⽤用奇异值分解的方法来计算\n",
    "    - `cholesky`: 采⽤用scipy.linalg.solve函数求得闭式解。\n",
    "    - `sparse_cg`: 采⽤用scipy.sparse.linalg.cg函数来求取最优解。\n",
    "    - `lsqr`: 使用scipy.sparse.linalg.lsqr 求解，它是最快的。\n",
    "    - `sag`: 使用随机平均梯度下降，当n_samples和n_features都较大时，通常比其他求解器更快。\n",
    "- `max_iter`: 最大迭代次数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T05:28:37.232305Z",
     "start_time": "2021-03-21T05:28:37.193616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:\t30.31898\n"
     ]
    }
   ],
   "source": [
    "model_L2 = Ridge()\n",
    "model_L2.fit(X_train, y_train)\n",
    "y_pred = model_L2.predict(X_test)\n",
    "print('MSE:\\t%.5f' % MSE(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 简单实现\n",
    "涉及梯度下降法，暂时还不在学习计划内，或许休息期会补上。现在不搞但以后总会搞的\t&#x1F609;\n",
    "\n",
    "    求解方法：\n",
    "    1. 梯度下降法\n",
    "    2. 标准方程法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sample_ridge():\n",
    "    def __init__(self, alpha=0.1, max_iter=10000):\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "my_lasso = sample_ridge(alpha=0.5)       "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso回归(L1)\n",
    "![./Image/Lasso.svg](./Image/Lasso.svg)\n",
    "**参考资料**：\n",
    "- [sklearn 中的线性回归、岭回归、Lasso回归参数配置及示例](https://blog.csdn.net/VariableX/article/details/107166602)\n",
    "- [Lasso回归算法： 坐标轴下降法与最小角回归法小结](https://www.cnblogs.com/pinard/p/6018889.html)\n",
    "- [手写算法-python代码实现Lasso回归](https://blog.csdn.net/weixin_44700798/article/details/110690015)\n",
    "- [从Lasso开始说起](https://zhuanlan.zhihu.com/p/46999826)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sklearn调用\n",
    "**sklearn参数**：\n",
    "- `alpha`: 正则项系数。数值越大，则对复杂模型的惩罚力度越大。\n",
    "        调参方法：\n",
    "        1. 给定alpha较小的值，例如0.1。\n",
    "        2. 根据验证集准确率以10倍为单位增大或者减小参数值。\n",
    "        3. 在找到合适的数量级后，在此数量级上微调。\n",
    "        合适的候选值：[0.001, 0.01, 0.1, 1, 10, 100]\n",
    "- `normalize`: 是否对各个特征进行标准化。（默认方式：减去均值并除以l2范数）\n",
    "        标准化的好处:\n",
    "        1. 加速收敛\n",
    "        2. 提升精度\n",
    "- `fit_intercept`: 是否计算截距。\n",
    "- `max_iter`: 最大迭代次数。\n",
    "- `selection`: 指定了每轮迭代时，选择权重向量的哪个分量来更新\n",
    "    - `random`: 更新的时候，随机选择权重向量的⼀个分量来更更新。\n",
    "    - `cyclic`: 更新的时候，从前向后依次选择权重向量的⼀个分量来更新。\n",
    "\n",
    "**注**：参数未全部列出，详细可见参考文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T02:26:07.313490Z",
     "start_time": "2021-03-20T02:26:07.300513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:\t34.17221\n"
     ]
    }
   ],
   "source": [
    "model_L1 = Lasso()\n",
    "model_L1.fit(X_train, y_train)\n",
    "y_pred = model_L1.predict(X_test)\n",
    "print('MSE:\\t%.5f' % MSE(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 简单实现\n",
    "暂时还不会。。慢慢来吧\n",
    "\n",
    "    求L1范数的损失函数极小值的解法：\n",
    "    1. 坐标轴下降法(coordinate descent)\n",
    "    2. 最小角回归法(Least Angle Regression,LARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T05:37:19.173893Z",
     "start_time": "2021-03-21T05:37:19.165845Z"
    }
   },
   "outputs": [],
   "source": [
    "class sample_lasso():\n",
    "    def __init__(self, alpha=0.1, max_iter=10000):\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "my_lasso = sample_lasso(alpha=0.5)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 降维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 主成分分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sklearn调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 简单实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "289.275px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
